#Depressive-Disorder_Prediction_Model-

## Overview

In this research project, surveyed data from the Risk Factor Surveillance System (BRFSS) for the year 2020 were analyzed to predict if a person has experienced a depressive disorder of some sort. The class labels predicting this factor are categorical, with “Yes” being represented “Y” and “No” being represented as “N.” The initial dataset contained 5000 rows with 276 attributes, however, the number of attributes after pre-processing reduced to 108 upon invoking a five-step procedure for it. After selecting two sampling methods, three feature selection algorithms, and six classification algorithms, thirty-six combinations that attempt to accurately predict the classes of each person surveyed were generated.


## Data Mining Tools 

The four main data mining tools used in the project are: 
Data Preprocessing – This involves understanding the data set better and reducing it to a more usable size where the data would work in a similar manner if all data points were used. 
Data Balancing – Balancing the dataset allows for the classes to be split in a fair manner (not allowing for any bias in the classification). This often involves under sampling and/or oversampling the dataset. 
Applying Feature Selection Methods – Feature Selection Methods allow for the most important features to be selected out of the many attributes that are present within the dataset. Like the data pre-processing step, this application allows for the overall computational time for the predictions to reduce. 
Data Classification – After the first three tools above are utilized to prepare the dataset for further analyses, the classification models to predict the classes are created using a combination of supervised machine learning algorithms. 

## DATA PRE-PROCESSING 

The data pre-processing was performed with a five-step procedure as follows: 
1) Deleting empty columns 
2) Deleting columns with zero variances 
3) Replacing Null Values with the Class Mean 
4) Deleting columns with outliers using the IQR method 
5) Deleting Hidden Columns and Values 

## Data Balancing

After the dataset was processed, the preprocessed dataset was split into two groups – namely the training set and the testing set. The training and testing set were split with an 80% - 20% ratio respectively.
Two types of sampling methods were undertaken to balance the data set, namely under-sampling and oversampling. Under-sampling focuses on deleting instances of the majority class to ensure that the model does not become biased when predicting new class labels from the testing set. Oversampling, on the other hand, focuses on replicating instances of the minority class to ensure that it is represented somewhat equally to its respective majority class. 
After this step, the under-sampled set contained 744 Ns and 761 Ys whereas the oversampled set had 3238 Ns and 3187 Ys. 

## Feature Selection Method

three methods were invoked to select the most important features from the dataset: 
1) Utilizing the CFS method 
2) Utilizing the Boruta method 
3) Calculating information gain 


## Data Classification
The following six classifiers were selected: 
1) Logistic Regression 
2) Naïve-Bayes 
3) Random Forest Classifier 
4) Decision Tree (ID3 Model) 
5) Information Gain (J48) 
6) KNN (K-Nearest Neighbor) Classifier 


## Conclusion
After reviewing the results generated by the discussion and results section, Random Forest Classifier produces the highest accuracy of 88% when under-sampled and over-sampled with all feature selection processes. 



